{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d19f5c46",
   "metadata": {},
   "source": [
    "# Deep Q-Network (DQN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b2272af",
   "metadata": {},
   "source": [
    "<div style=\"text-align:justify\">\n",
    "Deep Q-Network is the simplest architecture involving <b>Reinforcement Learning (RL)</b> and <b>Deep Neural Networks</b>. This is an introduction tutorial to play CartPole Game. Before starting, take a look at this <a href=\"https://www.youtube.com/watch?v=XiigTGKZfks\">YouTube video</a> with a real-life demonstration of a CartPole problem learning process.\n",
    "<br><br>\n",
    "RL is a type of machine learning that allow us to create AI agents that learn from the environment by interacting with it to maximize cumulative reward. In the same way, how we learn to ride a bicycle, AI learn it by trial and error, agents in RL algorithms are incentivized with punishments for wrong actions and rewards for good ones.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b497264",
   "metadata": {},
   "source": [
    "## Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b5266b8",
   "metadata": {},
   "source": [
    "<div style=\"text-align:justify\">\n",
    "TensorFlow make it simple to implement a neural network. With the code bellow, we'll create an empty neural network model. <b>Activation</b>, <b>loss</b> and <b>optimizer</b> are the parameters that define the characteristics of the neural networks, but <b>we are not going to discuss them here</b>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "35e9a257",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "import numpy, random, tensorflow\n",
    "import gymnasium as gym\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d6ab90ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_9\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_9\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)              │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_36 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,560</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_37 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">131,328</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_38 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">16,448</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_39 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">130</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_9 (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m)              │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_36 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │         \u001b[38;5;34m2,560\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_37 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │       \u001b[38;5;34m131,328\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_38 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │        \u001b[38;5;34m16,448\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_39 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)              │           \u001b[38;5;34m130\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">150,466</span> (587.76 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m150,466\u001b[0m (587.76 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">150,466</span> (587.76 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m150,466\u001b[0m (587.76 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def Brain(input_shape, action_shape):\n",
    "    _input = Input(shape=input_shape)\n",
    "    \n",
    "    _layer = Dense(512, activation='relu', kernel_initializer='he_uniform')(_input)\n",
    "    \n",
    "    _layer = Dense(256, activation='relu', kernel_initializer='he_uniform')(_layer)\n",
    "    \n",
    "    _layer = Dense(64, activation='relu', kernel_initializer='he_uniform')(_layer)\n",
    "    \n",
    "    _layer = Dense(action_shape, activation='linear', kernel_initializer='he_uniform')(_layer)\n",
    "    \n",
    "    _model = Model(inputs=_input, outputs=_layer)\n",
    "    \n",
    "    _optimizer = RMSprop(learning_rate=0.00025, rho=0.95, epsilon=0.01)\n",
    "    _model.compile(loss='mse', optimizer=_optimizer, metrics=['accuracy'])\n",
    "    \n",
    "    return _model\n",
    "\n",
    "# create model and show summary.\n",
    "Brain(input_shape=(4,), action_shape=2).summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ed1a5f",
   "metadata": {},
   "source": [
    "<div style=\"text-align:justify\">\n",
    "Later in the training process, you will see the neural network predict the reward value from a particular state. You will see that in code, I'll use <code>model.fit(next_state, reward)</code>. After training, the model will be able to predict the output from unseen input value.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "250c94e5",
   "metadata": {},
   "source": [
    "## Deep Q-Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a18194",
   "metadata": {},
   "source": [
    "<div style=\"text-align:justify\">\n",
    "Q-learning was introduced by Chris Watkins in 1989 and can be derived from the Bellman Equation.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d567a698",
   "metadata": {},
   "source": [
    "### Bellman Equation:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be3a78eb",
   "metadata": {},
   "source": [
    "- <i>s</i>: State\n",
    "- <i>a</i>: Action\n",
    "- <i>R</i>: Reward\n",
    "- <i>y</i>: Discount factor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e393df01",
   "metadata": {},
   "source": [
    "$\n",
    "V(s) = \\max_{a}(R(s,a) + yV(s'))\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad9c8ab",
   "metadata": {},
   "source": [
    "<i>Where</i>:\n",
    "- $\\max_{a}$: We choose the action that provides <i>maximum</i> reward.\n",
    "- $R(s,a)$: Take an action for a specific state.\n",
    "- $yV(s')$: Value of the next state."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d16e084c",
   "metadata": {},
   "source": [
    "### Markov Decision Process:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0e04af",
   "metadata": {},
   "source": [
    "We'll have more than one next possible state. Then:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "602265a6",
   "metadata": {},
   "source": [
    "$A.V(s'_1) + B.V(s'_2) + ... + N.V(s'_n)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e749709d",
   "metadata": {},
   "source": [
    "<i>A</i>, <i>B</i> and <i>N</i> are probabilities to one of the next possible state."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2613f57d",
   "metadata": {},
   "source": [
    "<i>Simplifying</i>:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d5065f",
   "metadata": {},
   "source": [
    "$V(s) = \\max_{a}(R(s,a) + y\\sum P(s, a, s')V(s'))$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a24542e",
   "metadata": {},
   "source": [
    "### Living penalty:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce7efe7",
   "metadata": {},
   "source": [
    "<div style=\"text-align:justify\">\n",
    "We are only rewarding the robot when it gets to the destination. Ideally, there should be a reward for every action the robot takes to help it better assess the quality of its actions. The rewards need not be always the same. But it is much better than having some amount reward for the actions than having no rewards at all. This idea is known as the living penalty.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "655769b0",
   "metadata": {},
   "source": [
    "### Q-Learning:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a5a2e0",
   "metadata": {},
   "source": [
    "$V(s) = \\max_{a}(R(s,a) + y\\sum P(s, a, s')V(s'))$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c987d45b",
   "metadata": {},
   "source": [
    "$Q(s,a) = R(s,a) + y\\sum P(s, a, s') \\max_{a} Q(s',a')$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68435ee",
   "metadata": {},
   "source": [
    "$Q(s,a) = R(s,a) + y \\max_{a} Q(s',a')$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "575825e0",
   "metadata": {},
   "source": [
    "### Temporal Difference:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f62c871b",
   "metadata": {},
   "source": [
    "<div style=\"text-align:justify\">\n",
    "Temporal difference is an agent learning from an environment through episodes with no prior knowledge of the environment. This means temporal difference takes a model-free or unsupervised learning approach. You can consider it learning from trial and error.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e26c7ab",
   "metadata": {},
   "source": [
    "<i>Prior value</i>:<br><br>\n",
    "$M = Q(s, a)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe885d07",
   "metadata": {},
   "source": [
    "<i>After iteration</i>:<br><br>\n",
    "$K = R(s,a) + y \\max_{a} Q(s',a')$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a61d8769",
   "metadata": {},
   "source": [
    "$TD(s,a) = K - M$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d419f571",
   "metadata": {},
   "source": [
    "$TD(s,a) = Q_{t-1}(s,a) + \\alpha TD_{t}(s,a)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b2635e1",
   "metadata": {},
   "source": [
    "## Remember Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e84ca3f6",
   "metadata": {},
   "source": [
    "<div style=\"text-align:justify\">\n",
    "One of the specific things for DQN is that the Neural Network used in the algorithm tends to forget the previous experiences as it overwrites them with new experiences. So, we need a memory of previous experiences and observations to re-train the model with the earlier experiences. We will call this array of experiences memory and use a <code>remember()</code> function to append state, action, reward, and next state to the memory.    \n",
    "</div>\n",
    "\n",
    "In our example, the memory list will have a form of:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e2e4f2",
   "metadata": {},
   "source": [
    "<code>memory = [(state, action, reward, next_state, done)...]</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d9838b1",
   "metadata": {},
   "source": [
    "<pre>\n",
    "<font color=\"#07a\">def</font> <font color=\"#DD4A68\">remember</font>(self, state, action, reward, next_state, done):\n",
    "    self.memory.append((state, action, reward, next_state, done))\n",
    "    <font color=\"#07a\">if</font> <font color=\"#690\">len</font>(self.memory) > self.train_start:\n",
    "        <font color=\"#07a\">if</font> self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5b068b",
   "metadata": {},
   "source": [
    "## Act"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdfb018d",
   "metadata": {},
   "source": [
    "<div style=\"text-align:justify\">\n",
    "Epsilon is the rate in which an agent randomly decides its action rather than a prediction.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00bde63",
   "metadata": {},
   "source": [
    "<pre>\n",
    "<font color=\"#07a\">def</font> <font color=\"#DD4A68\">act</font>(self, state):\n",
    "    <font color=\"#07a\">if</font> numpy.random.random() <= self.epsilon:\n",
    "        return random.randrange(self.action_size)\n",
    "    <font color=\"#07a\">else</font>:\n",
    "        return numpy.argmax(self.model.predict(state))\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4758bf91",
   "metadata": {},
   "source": [
    "## Replay Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c49d9fa",
   "metadata": {},
   "source": [
    "<div style=\"text-align:justify\">\n",
    "To make the agent perform well in the long term, we need to consider the immediate rewards and the future rewards we will get. To do this, we will have a <b>discount rate or alpha</b> and ultimately add it to the current state reward. This way, the agent will learn to maximize the discounted future reward based on the given state. In other words, we are updating our Q value with the cumulative discounted future rewards.\n",
    "<br><br>\n",
    "<code>done</code> is just a Boolean that indicates if the state is the final state (cartpole failed).\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26f03d6",
   "metadata": {},
   "source": [
    "<div style=\"text-align:justify\">\n",
    "    A method that trains NN with experiences in the memory we will call <code>replay()</code> function. First, we will sample some experiences from the memory and call them minibatch: <code>minibatch = random.sample(memory, min(len(memory), batch_size))</code>. I will set the batch size as 64 for this example. If the memory size is less than 64, we will take everything is in our memory.\n",
    "<br><br>\n",
    "For those of you who wonder how such function can converge, as it looks like it is trying to predict its output (in some sense it is), don't worry — it's possible, and in our simple case, it does. However, convergence is not always that easy, and in more complex problems, there comes a need for more advanced techniques than CartPole stabilize training. For example, these techniques are Double DQN's or Dueling DQN's, but that's a topic for another article (stay tuned).\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc9e8b74",
   "metadata": {},
   "source": [
    "## Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "655f0a27",
   "metadata": {},
   "source": [
    "Hyper Parameters:\n",
    "\n",
    "There are some parameters that have to be passed to a reinforcement learning agent. You will see similar parameters in all DQN models:\n",
    "\n",
    "- EPISODES: Number of games we want the agent to play;\n",
    "- ALPHA: Decay or discount rate, to calculate the future discounted reward;\n",
    "- EPSILON: Exploration rate is the rate in which an agent randomly decides its action rather than a prediction;\n",
    "- EPSILON_DECAY: We want to decrease the number of explorations as it gets good at playing games;\n",
    "- EPSILON_MIN: We want the agent to explore at least this amount;\n",
    "- LEARN_RATE — Determines how much neural net learns in each iteration (if used);\n",
    "- BATCH_SIZE — Determines how much memory DQN will use to train;\n",
    "- MEMORY_SIZE - Number of states in the memory;\n",
    "- MIN_BATCH_TO_TRAIN: Minimum batch size to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f843f12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Registra a função mse para carregar modelos HDF5 com 'mse' como loss\n",
    "@tensorflow.keras.utils.register_keras_serializable(package='custom', name='mse')\n",
    "def mse(y_true, y_pred):\n",
    "    return tensorflow.keras.losses.mean_squared_error(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8d9de00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        self.model_name = 'models/cartpole-dqn.h5'\n",
    "        \n",
    "        # Hyper parameters\n",
    "        self.MIN_BATCH_TO_TRAIN = 1000\n",
    "        self.EPSILON_DECAY = 0.999\n",
    "        self.EPSILON_MIN = 0.001\n",
    "        self.MEMORY_SIZE = 2000\n",
    "        self.EPISODES = 1000\n",
    "        self.BATCH_SIZE = 64\n",
    "        self.epsilon = 1.0\n",
    "        self.ALPHA = 0.95\n",
    "        \n",
    "        # Environment setup\n",
    "        self.env = gym.make('CartPole-v1')\n",
    "        self.state_size = self.env.observation_space.shape[0]\n",
    "        self.action_size = self.env.action_space.n\n",
    "        self.memory = deque(maxlen=self.MEMORY_SIZE)\n",
    "        \n",
    "        # Model\n",
    "        self.model = Brain(input_shape=(self.state_size,), action_shape=self.action_size)\n",
    "    \n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        \n",
    "        # Save state in memory\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "        \n",
    "        # Decrease epsilon value\n",
    "        if len(self.memory) > self.MIN_BATCH_TO_TRAIN:\n",
    "            if self.epsilon > self.EPSILON_MIN:\n",
    "                self.epsilon = self.epsilon * self.EPSILON_DECAY\n",
    "    \n",
    "    def act(self, state):\n",
    "        if numpy.random.random() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        else:\n",
    "            state = numpy.reshape(state, [1, self.state_size])\n",
    "            return numpy.argmax(self.model.predict(state))\n",
    "    \n",
    "    def replay(self):\n",
    "        \n",
    "        # Skip train\n",
    "        if len(self.memory) <= self.MIN_BATCH_TO_TRAIN:\n",
    "            return\n",
    "        \n",
    "        # Randomly sample from memory\n",
    "        minibatch = random.sample(self.memory, min(len(self.memory), self.BATCH_SIZE))\n",
    "        \n",
    "        # Get values from sample\n",
    "        _state, _action, _reward, _next_state, _done = map(numpy.array, zip(*minibatch))\n",
    "        \n",
    "        # Predict targets\n",
    "        target = self.model.predict(_state)\n",
    "        target_next = self.model.predict(_next_state)\n",
    "        \n",
    "        # Calculate Q-Values.\n",
    "        for i in range(self.BATCH_SIZE):\n",
    "            if _done[i]:\n",
    "                target[i][_action[i]] = _reward[i]\n",
    "            else:\n",
    "                target[i][_action[i]] = _reward[i] + (self.ALPHA * numpy.amax(target_next[i]))\n",
    "        \n",
    "        # Train model (single epoch)\n",
    "        with tensorflow.device('/GPU'):\n",
    "            self.model.fit(_state, target, batch_size=self.BATCH_SIZE, verbose=0)\n",
    "    \n",
    "    def train(self):\n",
    "        for i in range(self.EPISODES):\n",
    "            \n",
    "            # Reset before each epoch\n",
    "            _state, _ = self.env.reset()\n",
    "            _done = False\n",
    "            _lifetime = 0\n",
    "            \n",
    "            while not _done:\n",
    "                _action = self.act(_state)\n",
    "                _nxt_state, _reward, terminated, truncated, _info = self.env.step(_action)\n",
    "                _done = terminated or truncated\n",
    "                \n",
    "                # Calculate reward\n",
    "                if _done and _lifetime != self.env._max_episode_steps - 1:\n",
    "                    _reward = -100\n",
    "                \n",
    "                # Record experience\n",
    "                self.remember(_state, _action, _reward, _nxt_state, _done)\n",
    "                _state = _nxt_state\n",
    "                \n",
    "                _lifetime = _lifetime + 1\n",
    "                \n",
    "                if _done:\n",
    "                    msg = 'Episode: {}/{}, score: {}, epsilon: {:.5f}'\n",
    "                    print(msg.format(i, self.EPISODES, _lifetime, self.epsilon))\n",
    "                    \n",
    "                    # Save model and stop learning\n",
    "                    if _lifetime == self.env._max_episode_steps:\n",
    "                        self.model.save(self.model_name)\n",
    "                        return\n",
    "                \n",
    "                self.replay()\n",
    "    \n",
    "    def run(self):\n",
    "        # Load the trained model\n",
    "        try:\n",
    "            # Pass custom_objects to load the custom mse function\n",
    "            self.model = load_model(self.model_name, custom_objects={'mse': mse})\n",
    "            print(f\"Model loaded successfully from {self.model_name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading model: {e}\")\n",
    "            print(\"Please ensure the model has been trained and saved to the correct path.\")\n",
    "            return\n",
    "\n",
    "        print(\"Running the trained agent...\")\n",
    "        for i in range(10): # Run for 10 episodes\n",
    "            _state, _ = self.env.reset()\n",
    "            _done = False\n",
    "            _lifetime = 0\n",
    "\n",
    "            while not _done:\n",
    "                # The environment rendering is handled by render_mode='human'\n",
    "                # self.env.render() # This line is not needed with render_mode='human'\n",
    "\n",
    "                # Reshape state for model prediction\n",
    "                _state = numpy.reshape(_state, [1, self.state_size])\n",
    "\n",
    "                # Predict the best action using the loaded model\n",
    "                _action = numpy.argmax(self.model.predict(_state, verbose=0)) # verbose=0 to reduce output\n",
    "\n",
    "                # Take the action in the environment\n",
    "                _nxt_state, _reward, terminated, truncated, _info = self.env.step(_action)\n",
    "                _done = terminated or truncated\n",
    "\n",
    "                # Update state\n",
    "                _state = _nxt_state\n",
    "\n",
    "                _lifetime = _lifetime + 1\n",
    "\n",
    "                if _done:\n",
    "                    # Show score and stop the game\n",
    "                    msg = 'Episode: {}, score: {}'\n",
    "                    print(msg.format(i + 1, _lifetime)) # Use i + 1 for 1-based episode numbering\n",
    "                    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25698a40",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26c7767",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "agent = DQNAgent()\n",
    "#agent.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d5ea3e5",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aafb9908",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
